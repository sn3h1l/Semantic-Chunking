# -*- coding: utf-8 -*-
"""Task1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y0M56kVDhHaEVUlZ0Lh6RH1tCTuOF2E0

---
"""

# !pip install -q yt-dlp
# !pip install -q openai-whisper
# !pip install -q git+https://github.com/m-bain/whisperX.git
# !pip install -q sentence-transformers
# !pip install -q torch torchaudio
# !pip install -q spacy
# !python -m spacy download en_core_web_sm -q
# !apt-get install -qq ffmpeg

# print("All dependencies installed successfully!")

import torch
if torch.cuda.is_available():
    print(f"GPU Available: {torch.cuda.get_device_name(0)}")

else:
    print("No GPU detected - switching CPU")

import os
import gc
import json
import subprocess
import warnings
import numpy as np
from typing import List, Dict, Tuple, Optional
import torch
import torchaudio
from dataclasses import dataclass, field
from pathlib import Path
from IPython.display import Audio, display, HTML
import matplotlib.pyplot as plt
from tqdm import tqdm
warnings.filterwarnings('ignore')

# @markdown ### Configuration Parameters:
# @markdown Adjust these parameters based on your requirements

whisper_model_size = "medium" # @param ["tiny", "base", "small", "medium", "large", "large-v3"]
target_chunk_duration = 15 # @param {type:"slider", min:10, max:30, step:1}
similarity_threshold = 0.7 # @param {type:"slider", min:0.5, max:0.9, step:0.05}
save_audio_chunks = True # @param {type:"boolean"}

@dataclass
class Config:
    """Configuration for the semantic chunking pipeline"""
    # Model settings
    whisper_model: str = whisper_model_size
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # Audio settings
    sample_rate: int = 16000

    # VAD settings
    vad_threshold: float = 0.5
    min_speech_duration_ms: int = 250
    min_silence_duration_ms: int = 100

    # Chunking settings
    target_chunk_duration: float = float(target_chunk_duration)
    max_chunk_duration: float = target_chunk_duration + 5
    min_chunk_duration: float = 5.0
    similarity_threshold: float = similarity_threshold

    # Output settings
    save_audio_chunks: bool = save_audio_chunks

config = Config()
print(f"Configuration loaded: Whisper {config.whisper_model} on {config.device}")

# ============================================================================
# CELL 3: Core Functions
# ============================================================================

# @title Core Processing Functions { display-mode: "form" }
# @markdown This cell contains all the main processing functions

def download_youtube_audio(url: str) -> str:
    """Download audio from YouTube using yt-dlp"""
    import yt_dlp

    output_path = "youtube_audio.wav"

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': 'youtube_audio.%(ext)s',
        'quiet': False, # Changed to False for better visibility of yt-dlp output
        'no_warnings': True, # Suppress warnings
        'extractor_args': {"youtube": {"player_client": "android"}}, # Added to bypass bot detection
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'wav',
            'preferredquality': '192',
        }],
    }
    cookie_file="cookies.txt"
    if not os.path.isfile(cookie_file):
            raise FileNotFoundError(f"cookie_file not found: {cookie_file}")
    ydl_opts["cookiefile"] = cookie_file
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        video_title = info.get('title', 'Unknown')
        duration = info.get('duration', 0)

    print(f"Downloaded: {video_title}")
    print(f"Duration: {duration//60:.0f}m {duration%60:.0f}s")

    return output_path, video_title, duration


def apply_vad(audio_path: str) -> List[Dict]:
    """Apply Silero VAD to detect speech segments"""
    import torch
    import torchaudio
    
    silero_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False
    )

    get_speech_timestamps = utils[0]
    
    # Load audio manually to avoid compatibility issues
    try:
        wav, sr = torchaudio.load(audio_path)
        
        # Resample if needed
        if sr != 16000:
            resampler = torchaudio.transforms.Resample(sr, 16000)
            wav = resampler(wav)
        
        # Convert to mono if stereo
        if wav.shape[0] > 1:
            wav = torch.mean(wav, dim=0, keepdim=True)
        
        # Flatten to 1D tensor
        wav = wav.squeeze()
        
    except Exception as e:
        print(f"Error loading audio with torchaudio: {e}")
        print("Trying alternative loading method with librosa...")
        
        # Alternative: use librosa
        import librosa
        wav_array, _ = librosa.load(audio_path, sr=16000, mono=True)
        wav = torch.from_numpy(wav_array)

    speech_timestamps = get_speech_timestamps(
        wav,
        silero_model,
        sampling_rate=16000,
        threshold=config.vad_threshold,
        min_speech_duration_ms=config.min_speech_duration_ms,
        min_silence_duration_ms=config.min_silence_duration_ms,
        return_seconds=True
    )

    # Clean GPU memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return speech_timestamps


def transcribe_with_whisper(audio_path: str) -> Dict:
    """Transcribe using OpenAI Whisper"""
    import whisper

    print(f"Loading Whisper {config.whisper_model}...")
    model = whisper.load_model(config.whisper_model, device=config.device)

    print("Transcribing...")
    result = model.transcribe(
        audio_path,
        language="en",
        word_timestamps=True,
        temperature=0.0,
        beam_size=5,
        fp16=(config.device == "cuda"),
        verbose=False
    )

    # Clean up
    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

    return result


def enhance_with_whisperx(audio_path: str, whisper_result: Dict) -> List[Dict]:
    """Enhance alignment using WhisperX"""
    try:
        import whisperx

        print("Loading WhisperX for enhanced alignment...")
        device = config.device
        compute_type = "float16" if device == "cuda" else "float32"

        # Load audio
        audio = whisperx.load_audio(audio_path)

        # Load model
        model = whisperx.load_model(config.whisper_model, device, compute_type=compute_type)

        # Transcribe
        result = model.transcribe(audio, batch_size=2)

        # Align
        model_a, metadata = whisperx.load_align_model(
            language_code="en",
            device=device
        )

        aligned = whisperx.align(
            result["segments"],
            model_a,
            metadata,
            audio,
            device
        )

        # Clean up
        del model, model_a
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return aligned['segments']

    except Exception as e:
        print(f"WhisperX enhancement failed: {e}")
        print("Using native Whisper timestamps")
        return whisper_result['segments']


def semantic_chunk(segments: List[Dict]) -> List[Dict]:
    """Create semantic chunks from segments"""
    from sentence_transformers import SentenceTransformer
    import spacy

    # Load models
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    nlp = spacy.load("en_core_web_sm")

    chunks = []
    current_text = []
    current_start = None
    chunk_id = 1

    def calculate_similarity(text1: str, text2: str) -> float:
        """Calculate semantic similarity"""
        if not text1 or not text2:
            return 0.0
        emb1 = embedding_model.encode(text1)
        emb2 = embedding_model.encode(text2)
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return similarity

    for i, segment in enumerate(segments):
        # Get segment details
        seg_text = segment.get('text', '')
        seg_start = segment.get('start', 0)
        seg_end = segment.get('end', 0)

        # Add to current chunk
        if current_start is None:
            current_start = seg_start
        current_text.append(seg_text)

        # Calculate current duration
        current_duration = seg_end - current_start

        # Check if we should create a chunk
        should_chunk = False

        if current_duration >= config.max_chunk_duration:
            should_chunk = True
        elif current_duration >= config.target_chunk_duration - 5:
            # Check for semantic boundary
            if seg_text.strip().endswith(('.', '!', '?')):
                should_chunk = True
            elif i < len(segments) - 1:
                # Check semantic similarity with next segment
                next_text = segments[i + 1].get('text', '')
                if current_text and next_text:
                    sim = calculate_similarity(
                        ' '.join(current_text[-3:]),
                        next_text
                    )
                    if sim < config.similarity_threshold:
                        should_chunk = True

        # Create chunk if needed
        if should_chunk and current_duration >= config.min_chunk_duration:
            chunk_text = ' '.join(current_text).strip()

            chunks.append({
                "chunk_id": chunk_id,
                "chunk_length": round(current_duration, 2),
                "text": chunk_text,
                "start_time": round(current_start, 2),
                "end_time": round(seg_end, 2)
            })

            chunk_id += 1
            current_text = []
            current_start = None

    # Handle remaining text
    if current_text and current_start is not None:
        chunk_text = ' '.join(current_text).strip()
        duration = segments[-1].get('end', 0) - current_start

        if duration >= config.min_chunk_duration:
            chunks.append({
                "chunk_id": chunk_id,
                "chunk_length": round(duration, 2),
                "text": chunk_text,
                "start_time": round(current_start, 2),
                "end_time": round(segments[-1].get('end', 0), 2)
            })
        elif chunks:
            # Merge with previous chunk if too short
            chunks[-1]['text'] += ' ' + chunk_text
            chunks[-1]['end_time'] = round(segments[-1].get('end', 0), 2)
            chunks[-1]['chunk_length'] = round(
                chunks[-1]['end_time'] - chunks[-1]['start_time'], 2
            )

    return chunks

# ============================================================================
# CELL 4: Main Pipeline
# ============================================================================

# @title ðŸš€ Run Semantic Chunking Pipeline { display-mode: "form" }
# @markdown Enter the YouTube URL below and run the cell

youtube_url = "https://www.youtube.com/watch?v=Sby1uJ_NFIY" # @param {type:"string"}

def process_video(url: str):
    """Main processing pipeline"""

    print("=" * 60)
    print("ðŸŽ¬ YOUTUBE SEMANTIC CHUNKING PIPELINE")
    print("=" * 60)

    try:
        # Step 1: Download
        print("\nðŸ“¥ Step 1: Downloading YouTube audio...")
        audio_path, title, duration = download_youtube_audio(url)

        # Step 2: Convert to 16kHz mono
        print("\nðŸ”§ Step 2: Converting audio format...")
        processed_audio = "audio_16k.wav"
        subprocess.run([
            'ffmpeg', '-i', audio_path,
            '-ar', '16000', '-ac', '1',
            '-y', processed_audio
        ], capture_output=True)

        # Step 3: VAD
        print("\nðŸŽ™ï¸ Step 3: Applying Voice Activity Detection...")
        speech_segments = apply_vad(processed_audio)
        print(f"   Found {len(speech_segments)} speech segments")

        # Step 4: Transcription
        print("\nðŸ“ Step 4: Transcribing with Whisper...")
        transcription = transcribe_with_whisper(processed_audio)

        # Step 5: Enhanced alignment
        print("\nðŸŽ¯ Step 5: Enhancing timestamp alignment...")
        aligned_segments = enhance_with_whisperx(processed_audio, transcription)

        # Step 6: Semantic chunking
        print("\nðŸ§© Step 6: Creating semantic chunks...")
        chunks = semantic_chunk(aligned_segments)
        print(f"   Created {len(chunks)} chunks")

        # Step 7: Save results
        print("\nðŸ’¾ Step 7: Saving results...")
        with open('semantic_chunks.json', 'w', encoding='utf-8') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)

        # Optional: Extract audio chunks
        if config.save_audio_chunks and chunks:
            print("\nðŸŽµ Extracting audio chunks...")
            os.makedirs('audio_chunks', exist_ok=True)

            for chunk in chunks[:5]:  # First 5 chunks
                output_file = f"audio_chunks/chunk_{chunk['chunk_id']:03d}.wav"
                subprocess.run([
                    'ffmpeg', '-i', processed_audio,
                    '-ss', str(chunk['start_time']),
                    '-t', str(chunk['chunk_length']),
                    '-y', output_file
                ], capture_output=True)

        print("\n" + "=" * 60)
        print("âœ¨ PROCESSING COMPLETE!")
        print("=" * 60)

        return chunks, transcription, title

    except Exception as e:
        print(f"\nâŒ Error: {str(e)}")
        raise

# Run the pipeline
chunks, transcription, video_title = process_video(youtube_url)

# ============================================================================
# CELL 5: Display Results
# ============================================================================

# @title Display Results and Statistics { display-mode: "form" }

print(f"\nðŸ“¹ Video: {video_title}")
print(f"ðŸ”¢ Total Chunks: {len(chunks)}")
print(f"â±ï¸ Average Duration: {np.mean([c['chunk_length'] for c in chunks]):.1f}s")
print(f"ðŸ“ Duration Range: {min(c['chunk_length'] for c in chunks):.1f}s - "
      f"{max(c['chunk_length'] for c in chunks):.1f}s")

# Display first 5 chunks
print("\nðŸ“ Sample Chunks:")
print("-" * 60)

for chunk in chunks[:5]:
    print(f"\nðŸ”¹ Chunk {chunk['chunk_id']}:")
    print(f"   Time: [{chunk['start_time']:.1f}s - {chunk['end_time']:.1f}s] "
          f"({chunk['chunk_length']:.1f}s)")
    print(f"   Text: {chunk['text'][:150]}{'...' if len(chunk['text']) > 150 else ''}")

# Visualize chunk durations
if len(chunks) > 0:
    plt.figure(figsize=(12, 4))

    # Duration distribution
    plt.subplot(1, 2, 1)
    durations = [c['chunk_length'] for c in chunks]
    plt.hist(durations, bins=20, edgecolor='black', alpha=0.7)
    plt.axvline(config.target_chunk_duration, color='red', linestyle='--',
                label=f'Target ({config.target_chunk_duration}s)')
    plt.xlabel('Duration (seconds)')
    plt.ylabel('Count')
    plt.title('Chunk Duration Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Timeline visualization
    plt.subplot(1, 2, 2)
    for i, chunk in enumerate(chunks[:20]):  # First 20 chunks
        plt.barh(i, chunk['chunk_length'], left=chunk['start_time'],
                height=0.8, alpha=0.7)
    plt.xlabel('Time (seconds)')
    plt.ylabel('Chunk ID')
    plt.title('Chunk Timeline (First 20)')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ============================================================================
# CELL 6: Export and Download Results
# ============================================================================

# @title ðŸ’¾ Export Results for Submission { display-mode: "form" }

# Create downloadable JSON file
output_filename = 'semantic_chunks_output.json'

# Ensure proper format
formatted_output = []
for chunk in chunks:
    formatted_output.append({
        "chunk_id": chunk['chunk_id'],
        "chunk_length": chunk['chunk_length'],
        "text": chunk['text'],
        "start_time": chunk['start_time'],
        "end_time": chunk['end_time']
    })

# Save formatted output
with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(formatted_output, f, indent=2, ensure_ascii=False)

print(f"âœ… Results saved to: {output_filename}")
print(f"ðŸ“„ Total chunks: {len(formatted_output)}")

# Display download link in Colab
# from google.colab import files
# print("\nðŸ“¥ Download the results file:")
# files.download(output_filename)

# # If audio chunks were saved, create a zip file
# if config.save_audio_chunks and os.path.exists('audio_chunks'):
#     import zipfile

#     zip_filename = 'audio_chunks.zip'
#     with zipfile.ZipFile(zip_filename, 'w') as zipf:
#         for root, dirs, files_list in os.walk('audio_chunks'):
#             for file in files_list:
#                 zipf.write(os.path.join(root, file))

#     print(f"\nðŸŽµ Download audio chunks:")
#     files.download(zip_filename)

# ============================================================================
# CELL 6: Export and Download Results
# ============================================================================

# @title Export Results for Submission { display-mode: "form" }

# Create downloadable JSON file
output_filename = 'semantic_chunks_output.json'

# Ensure proper format
formatted_output = []
for chunk in chunks:
    formatted_output.append({
        "chunk_id": chunk['chunk_id'],
        "chunk_length": chunk['chunk_length'],
        "text": chunk['text'],
        "start_time": chunk['start_time'],
        "end_time": chunk['end_time']
    })

# Save formatted output
with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(formatted_output, f, indent=2, ensure_ascii=False)

print(f"âœ… Results saved to: {output_filename}")
print(f"ðŸ“„ Total chunks: {len(formatted_output)}")

# Display download link in Colab
# from google.colab import files
# print("\nðŸ“¥ Download the results file:")
# files.download(output_filename)

# # If audio chunks were saved, create a zip file
# if config.save_audio_chunks and os.path.exists('audio_chunks'):
#     import zipfile

#     zip_filename = 'audio_chunks.zip'
#     with zipfile.ZipFile(zip_filename, 'w') as zipf:
#         for root, dirs, files_list in os.walk('audio_chunks'):
#             for file in files_list:
#                 zipf.write(os.path.join(root, file))

#     print(f"\nðŸŽµ Download audio chunks:")
#     files.download(zip_filename)